{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nickname8888/Quaternions-LPI/blob/main/eng_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4MiUCQkWPmb"
      },
      "outputs": [],
      "source": [
        "!unzip '/content/drive/MyDrive/datasets/EnglishFnt.zip' -d '/content/drive/MyDrive/datasets/English5/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCiq9g6t0_qF"
      },
      "outputs": [],
      "source": [
        "# !unzip '/content/drive/MyDrive/datasets/Fnt58.zip' -d '/content/drive/MyDrive/datasets/English/Fnt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mymRsIG-XqRN",
        "outputId": "533a8900-fbf6-4a83-c028-92cbc5680b08"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "248063644451341145494649182395412689744530581492654164321720600128173828125"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "45**45"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6kmiBM5dWbH",
        "outputId": "5d6bb891-926e-407b-bc9d-e9b2c96621c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "62992\n"
          ]
        }
      ],
      "source": [
        "# import pathlib\n",
        "# data_dir = pathlib.Path('/content/drive/MyDrive/datasets/English/Fnt/')\n",
        "# image_count = len(list(data_dir.glob('*/*.png')))\n",
        "# print(image_count)\n",
        "# import os\n",
        "# content = os.listdir('/content/drive/MyDrive/datasets/English/Fnt/Sample061')\n",
        "# len(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzC7qQ2Hiaxr"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTR2oaRTjPN6"
      },
      "outputs": [],
      "source": [
        "#first making a discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1TjrAEOjRic"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0-Nzwc6jlBG"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "img_size = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aARSIvH0jq3p",
        "outputId": "29ef73bd-4238-4d55-8585-7aeb00543bc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 62992 files belonging to 62 classes.\n",
            "Using 50394 files for training.\n",
            "Found 62992 files belonging to 62 classes.\n",
            "Using 12598 files for validation.\n"
          ]
        }
      ],
      "source": [
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    '/content/drive/MyDrive/datasets/English/Fnt/',\n",
        "    batch_size = batch_size, image_size = (img_size, img_size),\n",
        "    seed=111, validation_split=0.2, subset='training'\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    '/content/drive/MyDrive/datasets/English/Fnt/',\n",
        "    batch_size = batch_size, image_size = (img_size, img_size),\n",
        "    seed=111, validation_split=0.2, subset='validation'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdKOeH2IktNy"
      },
      "outputs": [],
      "source": [
        "#normalizing dataset\n",
        "normalization_layer = layers.Rescaling(1./255)\n",
        "train_ds = train_ds.map(lambda xx, yy: (normalization_layer(xx), yy))\n",
        "val_ds = val_ds.map(lambda xx, yy: (normalization_layer(xx), yy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63FfBauGlhpm",
        "outputId": "25221451-a0a3-46b6-ad08-a94f4506d95f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_5 (Conv2D)           (None, 124, 124, 32)      2432      \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 61, 61, 64)        18496     \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 61, 61, 64)        0         \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 30, 30, 64)        36928     \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 30, 30, 64)        0         \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 14, 14, 128)       73856     \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 14, 14, 128)       0         \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 6, 6, 128)         147584    \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 6, 6, 128)         0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 4608)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               589952    \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 62)                7998      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 877,246\n",
            "Trainable params: 877,246\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "basic_discriminator_model = tf.keras.models.Sequential()\n",
        "basic_discriminator_model.add(layers.Conv2D(32, 5, 1, activation='relu', input_shape=(img_size, img_size, 3)))\n",
        "basic_discriminator_model.add(layers.Conv2D(64, 3, 2, activation='relu'))\n",
        "basic_discriminator_model.add(layers.Dropout(0.2))\n",
        "basic_discriminator_model.add(layers.Conv2D(64, 3, 2, activation='relu'))\n",
        "basic_discriminator_model.add(layers.Dropout(0.2))\n",
        "basic_discriminator_model.add(layers.Conv2D(128, 3, 2, activation='relu'))\n",
        "basic_discriminator_model.add(layers.Dropout(0.3))\n",
        "basic_discriminator_model.add(layers.Conv2D(128, 3, 2, activation='relu'))\n",
        "basic_discriminator_model.add(layers.Dropout(0.3))\n",
        "basic_discriminator_model.add(layers.Flatten())\n",
        "# basic_discriminator_model.add(layers.Dense(256, activation='relu'))\n",
        "# basic_discriminator_model.add(layers.Dropout(0.3))\n",
        "basic_discriminator_model.add(layers.Dense(128, activation='relu'))\n",
        "basic_discriminator_model.add(layers.Dropout(0.3))\n",
        "basic_discriminator_model.add(layers.Dense(62))\n",
        "\n",
        "basic_discriminator_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mEbUt3PmvJv",
        "outputId": "37665879-fac4-48b6-e467-d26091fe16fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "  24/1575 [..............................] - ETA: 18:04 - loss: 4.1315 - accuracy: 0.0182"
          ]
        }
      ],
      "source": [
        "basic_discriminator_model.compile('adam', tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "\n",
        "history = basic_discriminator_model.fit(train_ds, validation_data=val_ds, epochs=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOTnRju5ouLG"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EAS_lKDhrXD"
      },
      "outputs": [],
      "source": [
        "#now starting work on gan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKp87Ysf43ui",
        "outputId": "71fdabac-7814-4722-8f7b-8260931e88a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 62992 files belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    '/content/drive/MyDrive/datasets/English5/Fnt/',\n",
        "    batch_size = batch_size, image_size = (img_size, img_size),\n",
        "    label_mode=None,\n",
        "    # seed=111, validation_split=0.2, subset='training'\n",
        ")\n",
        "train_dataset = train_dataset.map(lambda x: x / 255.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5-w6utX5BnB",
        "outputId": "4c240a79-fe7b-425c-b45b-45924b06e0ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1969"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# for a, b in train_dataset:\n",
        "#   print(a)\n",
        "#   print(b)\n",
        "#   break\n",
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvpD5qyq8CKE"
      },
      "outputs": [],
      "source": [
        "dataset_new = train_dataset.take(500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "ol9MfLz96PGs",
        "outputId": "7acb1a1c-b51e-46a7-a53a-68e4381264e1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "for x in train_dataset:\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow((x.numpy() * 255).astype(\"int32\")[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X2Tbdni2TTG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/datasets/models/eng-gan-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Sh7c3J0htQO"
      },
      "outputs": [],
      "source": [
        "def create_discriminator():\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(layers.Conv2D(32, 5, 1, activation='relu', input_shape=(img_size, img_size, 3)))\n",
        "  model.add(layers.Conv2D(64, 3, 2, activation='relu'))\n",
        "  model.add(layers.Dropout(0.2))\n",
        "  model.add(layers.Conv2D(64, 3, 2, activation='relu'))\n",
        "  model.add(layers.Dropout(0.2))\n",
        "  model.add(layers.Conv2D(128, 3, 2, activation='relu'))\n",
        "  model.add(layers.Dropout(0.3))\n",
        "  model.add(layers.Conv2D(128, 3, 2, activation='relu'))\n",
        "  model.add(layers.Dropout(0.3))\n",
        "  model.add(layers.Flatten())\n",
        "  # model.add(layers.Dense(256, activation='relu'))\n",
        "  # model.add(layers.Dropout(0.3))\n",
        "  model.add(layers.Dense(128, activation='relu'))\n",
        "  model.add(layers.Dropout(0.3))\n",
        "  model.add(layers.Dense(1))\n",
        "  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1aI4-P02s8w",
        "outputId": "acd6662f-89e3-4726-be38-1f99acde1dcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 124, 124, 32)      2432      \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 61, 61, 64)        18496     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 61, 61, 64)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 30, 30, 64)        36928     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 30, 30, 64)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 14, 14, 128)       73856     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 14, 14, 128)       0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 6, 6, 128)         147584    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 6, 6, 128)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 4608)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               589952    \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 869,377\n",
            "Trainable params: 869,377\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "d = create_discriminator()\n",
        "d.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po8C3Di2iISa"
      },
      "outputs": [],
      "source": [
        "def create_generator():\n",
        "    model = tf.keras.Sequential()\n",
        "    \n",
        "    # creating Dense layer with units 7*7*256(batch_size) and input_shape of (100,)\n",
        "    model.add(layers.Dense(8*8, use_bias=False, input_shape=(100,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    # model.add(layers.Dense(7*7*4, use_bias=False, input_shape=(100,)))\n",
        "    # model.add(layers.BatchNormalization())\n",
        "    # model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Reshape((8, 8, 1)))\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(32, 3, strides=(1, 1), padding='same', use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, 3, strides=(2, 2), padding='same', use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, 3, strides=(2, 2), padding='same', use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, 3, strides=(2, 2), padding='same', use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, 3, strides=(2, 2), padding='same', use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(256, 3, strides=(2, 2), padding='same', use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2D(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_3V_CXb23Yu",
        "outputId": "7671d177-7c5a-4b2d-fcbe-c376276a4ba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_2 (Dense)             (None, 64)                6400      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 64)               256       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 64)                0         \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 8, 8, 1)           0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 8, 8, 32)         288       \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 8, 8, 32)         128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 8, 8, 32)          0         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 16, 16, 64)       18432     \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 16, 16, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2DT  (None, 32, 32, 64)       36864     \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 32, 32, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " conv2d_transpose_3 (Conv2DT  (None, 64, 64, 128)      73728     \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 64, 64, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 64, 64, 128)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_4 (Conv2DT  (None, 128, 128, 128)    147456    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 128, 128, 128)    512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 128, 128, 128)     0         \n",
            "                                                                 \n",
            " conv2d_transpose_5 (Conv2DT  (None, 256, 256, 256)    294912    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 256, 256, 256)    1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_6 (LeakyReLU)   (None, 256, 256, 256)     0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 128, 128, 3)       19200     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 600,224\n",
            "Trainable params: 598,752\n",
            "Non-trainable params: 1,472\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "g = create_generator()\n",
        "g.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbNW5bsCi-OV"
      },
      "outputs": [],
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def d_loss(real_output, fake_output):\n",
        "  real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "  fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "  return real_loss + fake_loss\n",
        "\n",
        "def g_loss(fake_output):\n",
        "  return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S95RlxrXmQmp"
      },
      "outputs": [],
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGnGgRarmji4"
      },
      "outputs": [],
      "source": [
        "noise_dim = 100\n",
        "num_of_generated_samples = 16\n",
        "seed = tf.random.normal([num_of_generated_samples, noise_dim])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyT_lSiVmqj8"
      },
      "outputs": [],
      "source": [
        "generator = create_generator()\n",
        "discriminator = create_discriminator()\n",
        "\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "  global generator, discriminator\n",
        "  noise = tf.random.normal([num_of_generated_samples, noise_dim])\n",
        "\n",
        "  with tf.GradientTape() as gtape, tf.GradientTape() as dtape:\n",
        "    generated_images = generator(noise, training=True)\n",
        "    real_output = discriminator(images, training=True)\n",
        "    fake_output = discriminator(generated_images, training=True)\n",
        "    gloss = g_loss(fake_output)\n",
        "    dloss = d_loss(real_output, fake_output)\n",
        "  generator_gradient = gtape.gradient(gloss, generator.trainable_variables)\n",
        "  discriminator_gradient = dtape.gradient(dloss, discriminator.trainable_variables)\n",
        "  generator_optimizer.apply_gradients(zip(generator_gradient, generator.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(discriminator_gradient, discriminator.trainable_variables))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4YXP2Wazjoz"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_gan(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    for image_batch in tqdm(dataset):\n",
        "      train_step(image_batch)\n",
        "    \n",
        "    noise = tf.random.normal([num_of_generated_samples, noise_dim])\n",
        "    generated_images = generator.predict(noise)\n",
        "    dim=(8,8)\n",
        "    figsize=(10,10)\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(generated_images.shape[0]):\n",
        "      plt.subplot(dim[0], dim[1], i+1)\n",
        "      plt.imshow(generated_images[i].reshape(128, 128, 3), interpolation='nearest')\n",
        "      plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('gan_generated_image %d.png' %epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umUJ-N3k2_8S",
        "outputId": "b540f777-9f8f-46e9-fac3-485546c20d9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [09:01<00:00,  1.08s/it]\n",
            "100%|██████████| 500/500 [09:07<00:00,  1.09s/it]\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "100%|██████████| 500/500 [09:00<00:00,  1.08s/it]\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "100%|██████████| 500/500 [09:01<00:00,  1.08s/it]\n",
            "100%|██████████| 500/500 [09:01<00:00,  1.08s/it]\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "100%|██████████| 500/500 [09:01<00:00,  1.08s/it]\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "100%|██████████| 500/500 [09:01<00:00,  1.08s/it]\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "100%|██████████| 500/500 [09:21<00:00,  1.12s/it]\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "100%|██████████| 500/500 [09:00<00:00,  1.08s/it]\n",
            "100%|██████████| 500/500 [09:00<00:00,  1.08s/it]\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "100%|██████████| 500/500 [09:01<00:00,  1.08s/it]\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "100%|██████████| 500/500 [09:01<00:00,  1.08s/it]\n",
            "100%|██████████| 500/500 [09:00<00:00,  1.08s/it]\n",
            " 39%|███▊      | 193/500 [03:28<05:23,  1.06s/it]"
          ]
        }
      ],
      "source": [
        "train_gan(dataset_new, 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUpNTimF3FNp"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "eng-gan.ipynb",
      "provenance": [],
      "mount_file_id": "1dSJHxKa_FKBdtkllDcUA0KhEIey55HhO",
      "authorship_tag": "ABX9TyMuvsp3OgKwjM9hSCOWlKtE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}